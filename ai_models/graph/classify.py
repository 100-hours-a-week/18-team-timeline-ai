from typing import List
from langgraph.graph import StateGraph, END, START
from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langchain_community.utilities import WikipediaAPIWrapper
from textwrap import dedent
from pydantic import BaseModel
from pprint import pprint
import logging

# ë¡œê·¸ ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜
system_prompt = dedent("""
You are an AI assistant designed to support Korean timeline-based question answering and comment classification.
You have access to the following tools:

1. `search_wiki`: Use this when a person's name or major entity is mentioned. It searches Wikipedia in Korean.
2. `search_web`: Use this if Wikipedia does not provide sufficient information. It represents a general web search.
3. `refine_timeline_card`: Use this to process a list of timeline events into a summarized document for further reasoning.

Follow these exact steps:

Step-by-step:
1. Read and understand the user comment or question.
2. If you need information not in the timeline, use one of the tools.
    - Use this format for tool calls:
        Action: <tool_name>
        Action Input: <input>
3. Wait for the tool output ("Observation") and cite the tool source:
        [Source: <tool_name> | <input summary> | <source url or internal>]
4. Repeat tool usage if necessary until you gather sufficient context.
5. Provide a final answer based on reasoning and tool results.

Constraints:
- Only use tools when necessary.
- Do not hallucinate or make claims without citation.
- Final answers must be concise, accurate, and cite all factual sources.

Example:
User: ê¹€ë²”ìˆ˜ëŠ” ëˆ„êµ¬ì•¼?

Action: search_wiki
Action Input: ê¹€ë²”ìˆ˜

Observation: ê¹€ë²”ìˆ˜ëŠ” ì¹´ì¹´ì˜¤ ì°½ì—…ìì´ë©° ì„œìš¸ëŒ€ ì¶œì‹ ì˜ ê¸°ì—…ê°€ì…ë‹ˆë‹¤...
[Source: search_wiki | ê¹€ë²”ìˆ˜ | https://ko.wikipedia.org/wiki/ê¹€ë²”ìˆ˜]

Final Answer: ê¹€ë²”ìˆ˜ëŠ” ì„œìš¸ëŒ€ ì¶œì‹ ì˜ ê¸°ì—…ê°€ë¡œ, ì¹´ì¹´ì˜¤ë¥¼ ì°½ì—…í•œ ì¸ë¬¼ì…ë‹ˆë‹¤.
""")

# ìƒíƒœ ì •ì˜
class TimelineState(BaseModel):
    timeline: List[str]

class TimelineResult(BaseModel):
    context_docs: List[str]

class AgenticCommentGraph:
    def __init__(self, server: str, model: str, max_retries: int = 3):
        self.max_retries = max_retries
        self.server = server
        self.model = model
        self.tools = [self.search_wiki, self.search_web, self.refine_timeline_card]

    @tool
    def search_wiki(self, query: str, k: int = 3) -> str:
        """ì¸ë¬¼ì´ ê¸€ì— í¬í•¨ì´ ë˜ì–´ìˆë‹¤ë©´ ìœ„í‚¤í”¼ë””ì•„ì—ì„œ ê²€ìƒ‰í•˜ì„¸ìš”."""
        wiki_search = WikipediaAPIWrapper(num_results=k, lang="ko")
        try:
            result = wiki_search.run(query)
        except Exception as e:
            raise RuntimeError(f"Error in search_wiki: {e}")
        return result

    @tool
    def search_web(self, query: str, k: int = 3) -> str:
        """TODO: ì›¹ ê²€ìƒ‰ ë„êµ¬ë¥¼ êµ¬í˜„í•˜ì„¸ìš”."""
        
        return "ì›¹ ê²€ìƒ‰ ê²°ê³¼"

    @tool
    def refine_timeline_card(self, state: TimelineState) -> TimelineResult:
        """íƒ€ì„ë¼ì¸ì„ ë°›ì•„ì„œ ì •ì œí•´ì„œ context ë¬¸ì„œë¡œ ë°˜í™˜"""
        document = "\n\n".join(state.timeline)
        logger.info(f"refine_timeline_card: {document}\n\n")
        return TimelineResult(context_docs=[document])

    def _make_llm(self):
        llm = ChatOpenAI(
            base_url=f"{self.server}/v1",
            api_key="not-needed",
            model=self.model,
            temperature=0.1,
            tool_choice="auto",
        )
        return llm

    def build(self):
        llm = self._make_llm()
        graph = create_react_agent(
            model=llm,
            tools=self.tools,
            prompt=system_prompt,
        )
        return graph

if __name__ == "__main__":
    SERVER = "https://8acc-34-125-119-95.ngrok-free.app"
    MODEL = "naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B"
    tmp = AgenticCommentGraph(server=SERVER, model=MODEL).build()
    print(tmp)

    inputs = {
        "input_text": "ì¹´ì¹´ì˜¤ ê¹€ë²”ìˆ˜ ì˜ì¥ì˜ ì°½ì—… ë°°ê²½ê³¼ ì‚¬íšŒ ê¸°ì—¬ì— ëŒ€í•´ ì•Œë ¤ì¤˜.",
        "timeline": [
            "ê¹€ë²”ìˆ˜ëŠ” ì„œìš¸ëŒ€í•™êµ ì‚°ì—…ê³µí•™ê³¼ë¥¼ ì¡¸ì—…í–ˆë‹¤.",
            "ì¹´ì¹´ì˜¤ë¥¼ ì°½ì—…í•˜ì—¬ í•œêµ­ ëª¨ë°”ì¼ ë©”ì‹ ì € ì‹œì¥ì„ ì„ ë„í–ˆë‹¤.",
            "ì¹´ì¹´ì˜¤ ì‚¬íšŒê³µí—Œì¬ë‹¨ì„ ì„¤ë¦½í•´ ë‹¤ì–‘í•œ ì‚¬íšŒì  í™œë™ì„ ì§€ì›í•˜ê³  ìˆë‹¤.",
        ],
        "score": 0,
        "worker_id": 42,
        "context_docs": [],
    }

    for step in tmp.stream(inputs):
        step_name = step.get('__step__') or next(iter(step.keys()), 'unknown')
        print(f"\nğŸ§© step: {step_name}")
        pprint(step.get('agent', step))
