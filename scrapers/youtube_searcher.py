import re, os
import pandas as pd
import dotenv
from typing import List
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from concurrent.futures import ThreadPoolExecutor, as_completed
from scrapers.base_searcher import BaseSearcher
from utils.exceptions import SearchRequestFailedError, InvalidQueryError
from scrapers.daum_vclip_searcher import DaumVclipSearcher
import asyncio
import aiohttp

class YouTubeCommentSearcher(BaseSearcher):
    """YouTube ëŒ“ê¸€ ìˆ˜ì§‘ê¸° (ë³‘ë ¬ ì§€ì›)"""

    def __init__(self, api_key: str, max_comments: int = 5, max_workers: int = 6):
        self.api_key = api_key
        self.max_comments = max_comments
        self.max_workers = max_workers

    @staticmethod
    def extract_video_id(url: str) -> str:
        """ìœ íŠœë¸Œ URLì—ì„œ ì˜ìƒ ID ì¶”ì¶œ"""
        match = re.search(r"v=([a-zA-Z0-9_-]+)", url)
        return match.group(1) if match else None

    def _fetch_comments(self, video_id: str) -> List[str]:
        """ê°œë³„ video_idì— ëŒ€í•´ ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸°"""
        youtube = build("youtube", "v3", developerKey=self.api_key)
        comments = []
        try:
            response = (
                youtube.commentThreads()
                .list(
                    part="snippet",
                    videoId=video_id,
                    textFormat="plainText",
                    maxResults=self.max_comments,
                )
                .execute()
            )

            for item in response["items"]:
                comment = item["snippet"]["topLevelComment"]["snippet"]
                author = comment.get("authorDisplayName", "ì•Œ ìˆ˜ ì—†ìŒ")
                text = comment.get("textDisplay", "")
                comments.append(text)

        except HttpError as e:
            raise SearchRequestFailedError(f"ID: {video_id} ìš”ì²­ ì‹¤íŒ¨: {e}")

        return comments

    def search(self, df: pd.DataFrame) -> List[str]:
        """DataFrame ê¸°ë°˜ ëŒ“ê¸€ ë³‘ë ¬ ìˆ˜ì§‘"""

        if df.empty or "url" not in df.columns:
            raise InvalidQueryError("DataFrameì´ ë¹„ì–´ìˆê±°ë‚˜ 'url' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

        ripple = []
        video_ids = []

        # ìœ íš¨í•œ ìœ íŠœë¸Œ IDë§Œ ì¶”ì¶œ
        for idx, series in df.iterrows():
            video_url = series["url"]
            video_id = self.extract_video_id(video_url)
            if video_id:
                video_ids.append(video_id)

        errors = []

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {
                executor.submit(self._fetch_comments, vid): vid for vid in video_ids
            }

            for future in as_completed(futures):
                video_id = futures[future]
                try:
                    comments = future.result()
                    ripple.extend(comments)
                    print(f"âœ… ID {video_id} ëŒ“ê¸€ {len(comments)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
                except SearchRequestFailedError as e:
                    print(f"âš ï¸ ìˆ˜ì§‘ ì‹¤íŒ¨ - {e}")
                    errors.append((video_id, str(e)))

        if errors:
            print("\nğŸš¨ ì˜¤ë¥˜ ë°œìƒ ì˜ìƒ ë¦¬ìŠ¤íŠ¸:")
            for video_id, error in errors:
                print(f"- {video_id}: {error}")

        return ripple
