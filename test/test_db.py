from qdrant_client import QdrantClient
from langchain_qdrant import QdrantVectorStore
from langchain.schema import Document
from datasets import load_dataset
import logging
import asyncio
import aiohttp
from typing import List
import time
import dotenv
import os
from scrapers.daum_vclip_searcher import DaumVclipSearcher
from scrapers.youtube_searcher import YouTubeCommentAsyncFetcher
from ai_models.graph.classify import ClassifyGraph
from ai_models.runner import Runner

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
LABELS = [
    "ë¶ˆí‰/ë¶ˆë§Œ",
    "í™˜ì˜/í˜¸ì˜",
    "ê°ë™/ê°íƒ„",
    "ì§€ê¸‹ì§€ê¸‹",
    "ê³ ë§ˆì›€",
    "ìŠ¬í””",
    "í™”ë‚¨/ë¶„ë…¸",
    "ì¡´ê²½",
    "ê¸°ëŒ€ê°",
    "ìš°ì­ëŒ/ë¬´ì‹œí•¨",
    "ì•ˆíƒ€ê¹Œì›€/ì‹¤ë§",
    "ë¹„ì¥í•¨",
    "ì˜ì‹¬/ë¶ˆì‹ ",
    "ë¿Œë“¯í•¨",
    "í¸ì•ˆ/ì¾Œì ",
    "ì‹ ê¸°í•¨/ê´€ì‹¬",
    "ì•„ê»´ì£¼ëŠ”",
    "ë¶€ë„ëŸ¬ì›€",
    "ê³µí¬/ë¬´ì„œì›€",
    "ì ˆë§",
    "í•œì‹¬í•¨",
    "ì—­ê²¨ì›€/ì§•ê·¸ëŸ¬ì›€",
    "ì§œì¦",
    "ì–´ì´ì—†ìŒ",
    "ì—†ìŒ",
    "íŒ¨ë°°/ìê¸°í˜ì˜¤",
    "ê·€ì°®ìŒ",
    "í˜ë“¦/ì§€ì¹¨",
    "ì¦ê±°ì›€/ì‹ ë‚¨",
    "ê¹¨ë‹¬ìŒ",
    "ì£„ì±…ê°",
    "ì¦ì˜¤/í˜ì˜¤",
    "íë­‡í•¨(ê·€ì—¬ì›€/ì˜ˆì¨)",
    "ë‹¹í™©/ë‚œì²˜",
    "ê²½ì•…",
    "ë¶€ë‹´/ì•ˆ_ë‚´í‚´",
    "ì„œëŸ¬ì›€",
    "ì¬ë¯¸ì—†ìŒ",
    "ë¶ˆìŒí•¨/ì—°ë¯¼",
    "ë†€ëŒ",
    "í–‰ë³µ",
    "ë¶ˆì•ˆ/ê±±ì •",
    "ê¸°ì¨",
    "ì•ˆì‹¬/ì‹ ë¢°",
]

# ê°ì • ë ˆì´ë¸” ë§¤í•‘ (KOTE ë°ì´í„°ì…‹ì˜ ì‹¤ì œ ë ˆì´ë¸” ê¸°ë°˜)
SENTIMENT_MAP = {
    # ê¸ì •
    "í™˜ì˜/í˜¸ì˜": "ê¸ì •",
    "ê°ë™/ê°íƒ„": "ê¸ì •",
    "ê³ ë§ˆì›€": "ê¸ì •",
    "ì¡´ê²½": "ê¸ì •",
    "ê¸°ëŒ€ê°": "ê¸ì •",
    "ë¿Œë“¯í•¨": "ê¸ì •",
    "í¸ì•ˆ/ì¾Œì ": "ê¸ì •",
    "ì‹ ê¸°í•¨/ê´€ì‹¬": "ê¸ì •",
    "ì¦ê±°ì›€/ì‹ ë‚¨": "ê¸ì •",
    "ê¹¨ë‹¬ìŒ": "ê¸ì •",
    "íë­‡í•¨(ê·€ì—¬ì›€/ì˜ˆì¨)": "ê¸ì •",
    "í–‰ë³µ": "ê¸ì •",
    "ê¸°ì¨": "ê¸ì •",
    "ì•ˆì‹¬/ì‹ ë¢°": "ê¸ì •",
    "ì•„ê»´ì£¼ëŠ”": "ê¸ì •",
    # ë¶€ì •
    "ë¶ˆí‰/ë¶ˆë§Œ": "ë¶€ì •",
    "ì§€ê¸‹ì§€ê¸‹": "ë¶€ì •",
    "ìŠ¬í””": "ë¶€ì •",
    "í™”ë‚¨/ë¶„ë…¸": "ë¶€ì •",
    "ìš°ì­ëŒ/ë¬´ì‹œí•¨": "ë¶€ì •",
    "ì•ˆíƒ€ê¹Œì›€/ì‹¤ë§": "ë¶€ì •",
    "ì˜ì‹¬/ë¶ˆì‹ ": "ë¶€ì •",
    "ë¶€ë„ëŸ¬ì›€": "ë¶€ì •",
    "ê³µí¬/ë¬´ì„œì›€": "ë¶€ì •",
    "ì ˆë§": "ë¶€ì •",
    "í•œì‹¬í•¨": "ë¶€ì •",
    "ì—­ê²¨ì›€/ì§•ê·¸ëŸ¬ì›€": "ë¶€ì •",
    "ì§œì¦": "ë¶€ì •",
    "ì–´ì´ì—†ìŒ": "ë¶€ì •",
    "íŒ¨ë°°/ìê¸°í˜ì˜¤": "ë¶€ì •",
    "ê·€ì°®ìŒ": "ë¶€ì •",
    "í˜ë“¦/ì§€ì¹¨": "ë¶€ì •",
    "ì£„ì±…ê°": "ë¶€ì •",
    "ì¦ì˜¤/í˜ì˜¤": "ë¶€ì •",
    "ë‹¹í™©/ë‚œì²˜": "ë¶€ì •",
    "ê²½ì•…": "ë¶€ì •",
    "ë¶€ë‹´/ì•ˆ_ë‚´í‚´": "ë¶€ì •",
    "ì„œëŸ¬ì›€": "ë¶€ì •",
    "ì¬ë¯¸ì—†ìŒ": "ë¶€ì •",
    "ë¶ˆì•ˆ/ê±±ì •": "ë¶€ì •",
    # ì¤‘ë¦½
    "ì—†ìŒ": "ì¤‘ë¦½",
    "ë¶ˆìŒí•¨/ì—°ë¯¼": "ì¤‘ë¦½",
    "ë†€ëŒ": "ì¤‘ë¦½",
    "ë¹„ì¥í•¨": "ì¤‘ë¦½",
}


class OllamaEmbeddings:
    def __init__(self, base_url="http://localhost:11434"):
        self.base_url = base_url
        self.model = "bge-m3"  # Ollamaì—ì„œ ì‚¬ìš©í•  ëª¨ë¸
        self.batch_size = 128  # ë°°ì¹˜ í¬ê¸° ì„¤ì •

async def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¹„ë™ê¸° ì„ë² ë”©í•©ë‹ˆë‹¤."""
        async with aiohttp.ClientSession() as session:
            all_embeddings = []
            for i in range(0, len(texts), self.batch_size):
                batch = texts[i : i + self.batch_size]
                embeddings = await self.embed_batch(session, batch)
                all_embeddings.extend(embeddings)
                logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {i + len(batch)}/{len(texts)}")
            return all_embeddings

    async def embed_query(self, text: str) -> List[float]:
        """ë‹¨ì¼ ì¿¼ë¦¬ë¥¼ ì„ë² ë”©í•©ë‹ˆë‹¤."""
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.base_url}/api/embeddings",
                json={"model": self.model, "prompt": text},
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    return data["embedding"]
                else:
                    error_text = await response.text()
                    raise Exception(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {error_text}")


def load_kote_dataset():
    """KOTE ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        ds = load_dataset("searle-j/kote", cache_dir=".dataset", trust_remote_code=True)
        logger.info(
            f"ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(ds['train'])} í•™ìŠµ ìƒ˜í”Œ, "
            f"{len(ds['test'])} í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ"
        )
        return ds
    except Exception as e:
        logger.error(f"ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise


def create_documents(dataset):
    """ë°ì´í„°ì…‹ì„ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    documents = []
    unique_labels = set()  # ê³ ìœ í•œ ë ˆì´ë¸” ìˆ˜ì§‘

    for split in ["test"]:
        for item in dataset[split]:
            # labelsê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²« ë²ˆì§¸ ê°ì • ì‚¬ìš©
            label_indices = (
                item["labels"][0]
                if isinstance(item["labels"], list)
                else item["labels"]
            )
            # ìˆ«ì ì¸ë±ìŠ¤ë¥¼ ì‹¤ì œ ë ˆì´ë¸”ë¡œ ë³€í™˜
            if isinstance(label_indices, list):
                labels = [LABELS[idx] for idx in label_indices]
                label = labels[0]  # ì²« ë²ˆì§¸ ë ˆì´ë¸” ì‚¬ìš©
            else:
                label = LABELS[label_indices]

            unique_labels.add(label)  # ê³ ìœ  ë ˆì´ë¸” ìˆ˜ì§‘
            doc = Document(
                page_content=item["text"],
                metadata={"label": label, "split": split, "id": item["ID"]},
            )
            documents.append(doc)

    logger.info(f"ì´ {len(documents)}ê°œì˜ ë¬¸ì„œ ìƒì„± ì™„ë£Œ")
    logger.info(f"ê³ ìœ í•œ ê°ì • ë ˆì´ë¸”: {sorted(unique_labels)}")
    return documents


async def process_and_store_batch(
    client: QdrantClient,
    collection_name: str,
    documents: List[Document],
    embeddings: List[List[float]],
    start_idx: int,
):
    """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê³  Qdrantì— ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        client: Qdrant í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤
        collection_name: ì €ì¥í•  ì»¬ë ‰ì…˜ ì´ë¦„
        documents: ì²˜ë¦¬í•  Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
        embeddings: ê° ë¬¸ì„œì˜ ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        start_idx: í˜„ì¬ ë°°ì¹˜ì˜ ì‹œì‘ ì¸ë±ìŠ¤
    """
    # Qdrantì— ì €ì¥í•  í¬ì¸íŠ¸ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    points = []
    for i, (doc, embedding) in enumerate(zip(documents, embeddings)):
        # ê° ë¬¸ì„œë¥¼ Qdrant í¬ì¸íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        points.append(
            {
                "id": start_idx + i,  # ê³ ìœ  ID (ì „ì²´ ë¬¸ì„œ ì¤‘ ìˆœì„œ)
                "vector": embedding,  # ì„ë² ë”© ë²¡í„°
                "payload": {  # ë©”íƒ€ë°ì´í„°
                    "text": doc.page_content,  # ì›ë³¸ í…ìŠ¤íŠ¸
                    "label": doc.metadata["label"],  # ê°ì • ë ˆì´ë¸”
                    "split": doc.metadata["split"],  # ë°ì´í„°ì…‹ ë¶„í• (train/val/test)
                    "id": doc.metadata["id"],  # ì›ë³¸ ë°ì´í„°ì…‹ ID
                },
            }
        )

    # ë¹„ë™ê¸°ë¡œ Qdrantì— ë°°ì¹˜ ì €ì¥
    # asyncio.to_threadë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
    await asyncio.to_thread(
        client.upsert, collection_name=collection_name, points=points
    )


async def update_comment_sentiment(
    client: QdrantClient, collection_name: str, comment_id: str, new_sentiment: str
):
    """ëŒ“ê¸€ì˜ ê°ì • ë¶„ë¥˜ë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤.

    Args:
        client: Qdrant í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤
        collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
        comment_id: ìˆ˜ì •í•  ëŒ“ê¸€ì˜ ID
        new_sentiment: ìƒˆë¡œìš´ ê°ì • ë¶„ë¥˜
    """
    try:
        # IDë¡œ ëŒ“ê¸€ ê²€ìƒ‰
        results = await asyncio.to_thread(
            client.scroll,
            collection_name=collection_name,
            scroll_filter={"must": [{"key": "id", "match": {"value": comment_id}}]},
        )

        if not results[0]:  # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°
            logger.error(f"IDê°€ {comment_id}ì¸ ëŒ“ê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False

        point = results[0][0]
        point_id = point.id

        # ê°ì • ë¶„ë¥˜ ì—…ë°ì´íŠ¸
        await asyncio.to_thread(
            client.set_payload,
            collection_name=collection_name,
            payload={"label": new_sentiment},
            points=[point_id],
        )

        logger.info(
            f"ëŒ“ê¸€ {comment_id}ì˜ ê°ì • ë¶„ë¥˜ê°€ '{new_sentiment}'ë¡œ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤."
        )
        return True

    except Exception as e:
        logger.error(f"ê°ì • ë¶„ë¥˜ ìˆ˜ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return False


def get_sentiment_from_similar_comments(results):
    """ìœ ì‚¬í•œ ëŒ“ê¸€ë“¤ì˜ ê°ì •ì„ ê¸°ë°˜ìœ¼ë¡œ ê°ì •ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤."""
    sentiment_counts = {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0}
    total_comments = len(results)

    for result in results:
        original_sentiments = result.payload["label"]
        # ë ˆì´ë¸”ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        if not isinstance(original_sentiments, list):
            original_sentiments = [original_sentiments]

        # ê° ëŒ“ê¸€ì˜ ê°ì •ì„ ì¹´ìš´íŠ¸
        comment_sentiments = {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0}
        for sentiment in original_sentiments:
            # ìˆ«ì ì¸ë±ìŠ¤ë¥¼ ì‹¤ì œ ë ˆì´ë¸”ë¡œ ë³€í™˜
            if isinstance(sentiment, (int, float)):
                try:
                    sentiment = LABELS[int(sentiment)]
                except (IndexError, ValueError):
                    logger.warning(f"ì˜ëª»ëœ ê°ì • ì¸ë±ìŠ¤: {sentiment}")
                    continue

            # ë ˆì´ë¸”ì´ ë§¤í•‘ì— ì—†ëŠ” ê²½ìš° ë¡œê¹…
            if sentiment not in SENTIMENT_MAP:
                logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” ê°ì • ë ˆì´ë¸”: {sentiment}")
                continue

            mapped_sentiment = SENTIMENT_MAP[sentiment]
            comment_sentiments[mapped_sentiment] += 1

        # í•´ë‹¹ ëŒ“ê¸€ì˜ ê°€ì¥ ë§ì€ ê°ì •ì„ ì„ íƒ
        max_sentiment = max(comment_sentiments.items(), key=lambda x: x[1])[0]
        sentiment_counts[max_sentiment] += 1

    # ê°ì • ë¹„ìœ¨ ê³„ì‚°
    sentiment_percentages = {
        sentiment: (count / total_comments) * 100 if total_comments > 0 else 0
        for sentiment, count in sentiment_counts.items()
    }

    # ê°€ì¥ ë§ì€ ê°ì •ê³¼ ë¹„ìœ¨ ë°˜í™˜
    max_sentiment = max(sentiment_counts.items(), key=lambda x: x[1])[0]
    return max_sentiment, sentiment_percentages, sentiment_counts


async def main():
    # 1. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
    embedding_model = OllamaEmbeddings()

    # 2. Qdrant í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
    client = QdrantClient(host="localhost", port=6333)

    # 3. ë°ì´í„°ì…‹ ë¡œë“œ
    dataset = load_kote_dataset()
    documents = create_documents(dataset)

    # 4. Qdrant ì»¬ë ‰ì…˜ ìƒì„±
    collection_name = "kote_comments"
    vector_size = 1024

    try:
        # ì»¬ë ‰ì…˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        collections = client.get_collections()
        collection_exists = any(
            col.name == collection_name for col in collections.collections
        )

        if collection_exists:
            logger.info(f"ê¸°ì¡´ ì»¬ë ‰ì…˜ '{collection_name}' ì‚¬ìš©")

            # YouTube ëŒ“ê¸€ ê²€ìƒ‰
            dotenv.load_dotenv(override=True)
            YOUTUBE_API_KEY = os.getenv("YOUTUBE_API_KEY")
            REST_API_KEY = os.getenv("REST_API_KEY")
            video_searcher = DaumVclipSearcher(api_key=REST_API_KEY)
            youtube_searcher = YouTubeCommentAsyncFetcher(
                api_key=YOUTUBE_API_KEY, max_comments=20
            )
            start_time = time.time()
            df = video_searcher.search("ìœ¤ì„ì—´ ìœ íŠœë¸Œ")
            test_queries = await youtube_searcher.search(df=df)

            # ì „ì²´ ëŒ“ê¸€ì˜ ê°ì • í†µê³„
            total_sentiment_counts = {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0}
            total_comments = 0

            for query in test_queries:
                print(f"\nğŸ” ê²€ìƒ‰ì–´: {query['comment']}")
                embedding = await embedding_model.embed_query(query["comment"])
                results = await asyncio.to_thread(
                    client.search,
                    collection_name=collection_name,
                    query_vector=embedding,
                    limit=5,  # ìƒìœ„ 5ê°œ ìœ ì‚¬ ëŒ“ê¸€
                )

                print(
                    f"ì›ë³¸ ëŒ“ê¸€: {query['comment']} | "
                    f"ì±„ë„: {query.get('channel', 'N/A')} | "
                    f"ì¢‹ì•„ìš”: {query.get('like_count', 0)}"
                )

                # ìœ ì‚¬ ëŒ“ê¸€ë“¤ì˜ ê°ì •ì„ ê¸°ë°˜ìœ¼ë¡œ ê°ì • ë¶„ë¥˜
                sentiment, percentages, counts = get_sentiment_from_similar_comments(
                    results
                )
                print(f"ë¶„ë¥˜ëœ ê°ì •: {sentiment}")
                print("ê°ì • ë¶„í¬:")
                for s, p in percentages.items():
                    print(f"  {s}: {p:.1f}%")
                    total_sentiment_counts[s] += counts[s]
                total_comments += len(results)

                print("\nìœ ì‚¬ ëŒ“ê¸€:")
                for result in results:
                    print(
                        f"ìœ ì‚¬ë„: {result.score:.4f} | "
                        f"ê°ì •: {result.payload['label']} | "
                        f"ë‚´ìš©: {result.payload['text']}"
                    )

            # ì „ì²´ í†µê³„ ì¶œë ¥
            print("\nğŸ“Š ì „ì²´ ëŒ“ê¸€ ê°ì • í†µê³„:")
            for sentiment, count in total_sentiment_counts.items():
                percentage = (count / total_comments) * 100 if total_comments > 0 else 0
                print(f"{sentiment}: {count}ê°œ ({percentage:.1f}%)")
            end_time = time.time()
            logger.info(f"í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ì™„ë£Œ! ì†Œìš” ì‹œê°„: {end_time - start_time:.1f}ì´ˆ")
            return
        else:
            # ì»¬ë ‰ì…˜ì´ ì—†ì„ ê²½ìš°ì—ë§Œ ìƒì„±
            client.create_collection(
                collection_name=collection_name,
                vectors_config={"size": vector_size, "distance": "Cosine"},
            )
            logger.info(f"ìƒˆ ì»¬ë ‰ì…˜ '{collection_name}' ìƒì„± ì™„ë£Œ")
    except Exception as e:
        logger.error(f"ì»¬ë ‰ì…˜ í™•ì¸/ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise

    # 5. ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¬¸ì„œ ì²˜ë¦¬ ë° ì €ì¥
    batch_size = 128  # í•œ ë²ˆì— ì²˜ë¦¬í•  ë¬¸ì„œ ìˆ˜
    start_time = time.time()

    # ì „ì²´ ë¬¸ì„œë¥¼ ë°°ì¹˜ í¬ê¸°ë§Œí¼ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
    for i in range(0, len(documents), batch_size):
        # í˜„ì¬ ë°°ì¹˜ì˜ ë¬¸ì„œ ì¶”ì¶œ
        batch_docs = documents[i : i + batch_size]
        # ë°°ì¹˜ ë‚´ ë¬¸ì„œë“¤ì˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
        texts = [doc.page_content for doc in batch_docs]

        # í˜„ì¬ ë°°ì¹˜ì˜ ì„ë² ë”© ìƒì„± (ë¹„ë™ê¸°)
        embeddings = await embedding_model.embed_documents(texts)

        # ìƒì„±ëœ ì„ë² ë”©ì„ Qdrantì— ì¦‰ì‹œ ì €ì¥ (ë¹„ë™ê¸°)
        await process_and_store_batch(
            client, collection_name, batch_docs, embeddings, i
        )

        # ì§„í–‰ ìƒí™© ë¡œê¹…
        logger.info(
            f"ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {i + len(batch_docs)}/{len(documents)} "
            f"({(i + len(batch_docs))/len(documents)*100:.1f}%)"
        )

    # ì „ì²´ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚° ë° ë¡œê¹…
    end_time = time.time()
    logger.info(f"ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ! ì†Œìš” ì‹œê°„: {end_time - start_time:.1f}ì´ˆ")


if __name__ == "__main__":
    asyncio.run(main())
