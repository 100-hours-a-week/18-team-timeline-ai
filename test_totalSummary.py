import logging
import time
from pprint import pprint

from ai_models.runner import Runner
from scrapers.article_extractor import ArticleExtractor
from ai_models.graph.total_summary import TotalSummarizationGraph
from ai_models.graph.Summary import SummarizationGraph
import dotenv

logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(name)s: %(message)s")


def test_summary_graph():
    URLS = [
        {
            "url": "https://www.hani.co.kr/arti/society/society_general/1192251.html",
            "title": "ì¡°ì„ ì¼ë³´",
        },
        {
            "url": "https://www.hani.co.kr/arti/society/society_general/1192255.html",
            "title": "ì¡°ì„ ì¼ë³´",
        },
        {"url": "https://www.hankyung.com/article/2025041493977", "title": "í•œê²¨ë ˆ"},
        {"url": "https://www.khan.co.kr/article/202504141136001", "title": "ê²½í–¥ì‹ ë¬¸"},
        {"url": "https://www.mk.co.kr/news/politics/11290687", "title": "ë§¤ì¼ê²½ì œ"},
        {
            "url": "https://www.chosun.com/politics/politics_general/2025/04/14/THWVKUHQG5CKFJF6CLZLP5PKM4",
            "title": "ì¡°ì„ ì¼ë³´",
        },
    ]
    SERVER = "http://35.216.120.155:8001"
    MODEL = "models/HyperCLOVAX-SEED-Text-Instruct-1.5B"

    logging.info("ğŸ“¦ ëª¨ë¸ ë° ê·¸ë˜í”„ ì´ˆê¸°í™” ì¤‘...")
    graph = SummarizationGraph(SERVER, MODEL).build()
    graph_total = TotalSummarizationGraph(SERVER, MODEL).build()

    logging.info("ğŸ“° ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì‹œì‘...")
    extractor = ArticleExtractor()
    start_time = time.time()
    len_title = 0
    try:
        articles = extractor.search(urls=URLS)
        logging.info(articles)
        for article in articles:
            len_title += len(article["title"])
        len_title /= len(articles)
        logging.info(f"í‰ê·  ì œëª© ê¸¸ì´: {len_title}")
    except Exception as e:
        logging.exception("âŒ ê¸°ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨:")
        return
    logging.info(
        f"âœ… {len(articles)}ê°œ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì™„ë£Œ (ì†Œìš” ì‹œê°„: {time.time() - start_time:.2f}s)"
    )
    exit(0)
    len_text = 0
    logging.info("ğŸ“„ 1ì°¨ ìš”ì•½(ê°œë³„ ê¸°ì‚¬) ì‹œì‘...")
    runner = Runner(graph=graph)
    first_results = runner.run(texts=articles)
    for i, res in enumerate(first_results):
        len_text += len(res["text"])
        logging.info(f"ğŸ“ [1ì°¨ ì œëª© {i+1}] {articles[i]['title']}")
        logging.info(f"ğŸ“ [1ì°¨ ê²°ê³¼ {i+1}] {res['text'][:60]}...")
        logging.info(f"ğŸ“ [1ì°¨ ë³¸ë¬¸ {i+1}] {res['input_text'][:60]}...")
    len_text /= len(first_results)
    logging.info(f"í‰ê·  ë³¸ë¬¸ ê¸¸ì´: {len_text}")
    summarized_texts = [r["text"] for r in first_results]
    summarized_texts = {"input_text": "\n\n".join(summarized_texts)}

    logging.info("ğŸ“š 2ì°¨ ìš”ì•½(í†µí•© ìš”ì•½) ì‹œì‘...")
    final_runner = Runner(graph=graph_total)
    final_results = final_runner.run(texts=[summarized_texts])
    len_text_final = len(final_results[0]["summary"])
    len_title_final = len(final_results[0]["title"])
    logging.info(f"í‰ê·  ìµœì¢… ìš”ì•½ ê¸¸ì´: {len_text_final}")
    logging.info(f"ğŸ“ [2ì°¨ ê²°ê³¼] {len_title_final}...")
    logging.info("âœ… ìµœì¢… í†µí•© ìš”ì•½ ê²°ê³¼:")
    logging.info(final_results)
    pprint(final_results)


if __name__ == "__main__":
    dotenv.load_dotenv(override=True)
    test_summary_graph()
